{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a probability distribution $\\pi$, the goal of MCMC is to simulate a random variable $X$ whose distribution is $\\pi$.\n",
    "\n",
    "The MCMC algorithm constructs an ergodic (irreducible, aperiodic, and all states have finite expected return times) Markov chain whose limiting distribution is the desired $\\pi$.\n",
    "\n",
    "We can run the chain long enough so that the chain converges, or nearly converges, to its limiting distribution. Then we can output the final element(s) of the Markov sequence as a sample from $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Y_1, Y_2, \\dots$ be an i.i.d. sequence with common mean $\\mu < \\infty$. The strong LLN states that, with probability 1,\n",
    "\n",
    "$$\\lim_{n\\to\\infty}\\frac{Y_1 + \\dots + Y_n}{n} = \\mu.$$\n",
    "\n",
    "Let $Y$ be a random variable with the same distribution as the $Y_i$ and assume $r$ is a bounded, real-valued function. Then, $r(Y_1), r(Y_2), \\dots$ is also an i.i.d. sequence with finite mean, and, with probability 1,\n",
    "\n",
    "$$\\lim_{n\\to\\infty}\\frac{r(Y_1) + \\dots + r(Y_n)}{n} = E(r(Y)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong Law of Large Numbers for Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $X_0, X_1, \\dots$ is an ergodic Markov chain with stationary distribution $\\pi$. Let $r$ be a bounded, real-valued function. Let $X$ be a random variable with distribution $\\pi$.\n",
    "\n",
    "Then, with probability 1,\n",
    "\n",
    "$$\\lim_{n\\to\\infty}\\frac{r(X_1) + \\dots + r(X_n)}{n} = E(r(X)),$$\n",
    "\n",
    "where $E(r(X)) = \\sum_{j} r(j)\\pi_j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consequences of Strong LLN for Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an ergodic Markov chain with stationary distribution $\\pi$, let $A$ be a nonempty subset of the state space.\n",
    "\n",
    "We can write $\\pi_A = \\sum_{j \\in A}\\pi_j$ and we can interpret $\\pi_A$ as the long-term average number of visits to $A$ of an ergodic Markov chain.\n",
    "\n",
    "Now, define the indicator variable\n",
    "\n",
    "$$I_A(x) = \\begin{cases} 1, &\\mbox{if } x \\in A, \\\\ 0, & \\mbox{if } x \\notin A. \\end{cases}$$\n",
    "\n",
    "Then, $\\sum_{k = 0}^{n-1} I_A(X_k)$ is the number of visits to $A$ in the first $n$ steps of the chain.\n",
    "\n",
    "Let X be a random variable with distribution $\\pi$. With probability 1,\n",
    "\n",
    "$$\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{k = 0}^{n-1} I_A(X_k) = E(I_A(X)) = P(X \\in A) = \\pi_A.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Binary Sequences w/ No Adjacent 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider sequences of length $m$ consisting of 0s and 1s. A sequence is \"good\" if it has no adjacent 1s.\n",
    "\n",
    "<b>Question:</b> What is the expected number of 1s in a good sequence if all good sequences are equally likely?\n",
    "\n",
    "For general $m$, the expected number of 1s is $\\mu = \\sum_{k}k\\pi_k$, where $\\pi_k$ is the probability that a good sequence of length $m$ has exactly $k$ 1s.\n",
    "\n",
    "There is no simple closed form expression for $\\mu$. So, we use a simulation.\n",
    "\n",
    "<b>The simulation:</b>\n",
    "\n",
    "If $x$ is a sequence of 0s and 1s of length $m$, let $r(x)$ be the number of 1s in the sequence. Assume we can generate a uniformly random sequence of length $m$ with no adjacent 1s. Doing this repeatedly and independently, generating good sequences $Y_1, Y_2, \\dots, Y_n$, we can generate a Monte Carlo estimate of the desired expectation with\n",
    "\n",
    "$$\\mu \\approx \\frac{r(Y_1) + r(Y_2) + \\dots + r(Y_n)}{n}$$\n",
    "\n",
    "for large $n$. This is by the LLN.\n",
    "\n",
    "Thus, the problem of estimating $\\mu$ reduces itself to the problem of simulating a good sequence.\n",
    "\n",
    "<b>Generating a good sequence:</b>\n",
    "\n",
    "The idea is to construct an ergodic Markov chain $X_0, X_1, \\dots$ whose state space is the set of good sequences and whose limiting distribution is uniform on the set of good sequences.\n",
    "\n",
    "The Markov chain is then generated and, as in the i.i.d. case, we take\n",
    "\n",
    "$$\\mu \\approx \\frac{r(X_1) + r(X_2) + \\dots + r(X_n)}{n}$$\n",
    "\n",
    "for large $n$, as an MCMC estimate for the desired expectation.\n",
    "\n",
    "<b>Constructing the Markov chain:</b>\n",
    "\n",
    "The Markov chain is constructed as a random walk on a graph whose vertices are good sequences. Proceed as follows.\n",
    "\n",
    "From a given good sequence, pick one of its $m$ components uniformly at random.\n",
    "\n",
    "If the component is 1, then switch it to 0. The new sequence is also a good sequence. Move to the new sequence.\n",
    "\n",
    "If the component is 0, then switch it to 1 only if the resulting sequence is good. If it is, then move to the new sequence. Otherwise, stay put and the walk stays at the current state.\n",
    "\n",
    "<b>Implementing the random walk algorithm:</b>\n",
    "\n",
    "Starting with the sequence of all 0s as the initial state of the chain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.90368\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = 100\n",
    "n = 100000\n",
    "seq = [0 for x in range(m)]\n",
    "chainSeq = [seq]\n",
    "mu = 0\n",
    "for i in range(n):\n",
    "    randInt = np.random.randint(0,m)\n",
    "    if seq[randInt] == 1:\n",
    "        seq[randInt] = 0\n",
    "    else:\n",
    "        if randInt == 0:\n",
    "            if seq[randInt+1] == 0:\n",
    "                seq[randInt] = 1\n",
    "        elif randInt == (m-1):\n",
    "            if seq[randInt-1] == 0:\n",
    "                seq[randInt] = 1\n",
    "        else:\n",
    "            if seq[randInt+1] == 0 and seq[randInt-1] == 0:\n",
    "                seq[randInt] = 1\n",
    "    chainSeq.append(seq)\n",
    "    mu += sum(seq)\n",
    "mu /= n\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above value is the MCMC estimate for the expected number of 1s, $\\mu$. An exact analysis for $m = 100$ gives $\\mu = 27.7921$.\n",
    "\n",
    "Note that the uniform distribution on the set of good sequences assigns probability $\\frac{1}{c}$ to each sequence, where $c$ is the number of good sequences. The actual value of $c$ was never needed in this implementation and could have even been unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis-Hastings Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\pi = (\\pi_1, \\pi_2, \\dots)$ be a discrete probability distribution. The algorithm constructs a reversible Markov chain $X_0, X_1, \\dots$ whose stationary distribution is $\\pi$.\n",
    "\n",
    "Let $T$ be a transition matrix for any irreducible Markov chain with the same state space as $\\pi$. We assume we know how to sample from $T$. The $T$ chain will be used as a proposal chain, generating elements of a sequence that the algorithm decides whether or not to accept.\n",
    "\n",
    "To describe the transition mechanism for $X_0, X_1, \\dots$, assume that at time $n$ the chain is at state $i$. The next step of the chain $X_{n+1}$ is determined by a two-step procedure:\n",
    "\n",
    "1. Choose a new state according to $T$. That is, choose $j$ with probability $T_{ij}$. State $j$ is called the proposal state.\n",
    "\n",
    "2. Decide whether or not to accept $j$ or not. Let\n",
    "\n",
    "    $$a(i,j) = \\frac{\\pi_j T_{ji}}{\\pi_i T_{ij}}.$$\n",
    "\n",
    "    The function $a$ is called the acceptance function. If $a(i,j) \\geq 1$, then $j$ is accepted at the state of the chain. If $a(i,j) < 1$, then $j$ is accepted with probability $a(i,j)$. If $j$ is not accepted, then $i$ is kept at the next state of the chain.\n",
    "\n",
    "In other words, assume $X_n = i$. Let $U$ be uniformly distributed on $(0,1)$. Then, set\n",
    "\n",
    "$$X_{n+1}(x) = \\begin{cases} j, &\\mbox{if } U \\leq a(i,j), \\\\ i, & \\mbox{if } U > a(i,j). \\end{cases}$$\n",
    "\n",
    "The sequence $X_0, X_1, \\dots$ constructed by th Metropolis-Hastings algorithm is a reversible Markov chain whose stationary distribution is $\\pi$.\n",
    "\n",
    "<b>Note:</b>\n",
    "\n",
    "1. The exact form of $\\pi$ is not necessary to implement this algorithm. It only uses ratios of the form $\\pi_j/\\pi_i$ and thus $\\pi$ only needs to be specified up to proportionality. If $\\pi$ is uniform on a set of size $c$, then $\\pi_j/\\pi_i = 1$ and the acceptance function reduces to $a(i,j) = T_{ji}/T_{ij}$.\n",
    "\n",
    "2. If the proposal transition matrix $T$ is symmetric, then $a(i,j) = \\pi_j/\\pi_i$.\n",
    "\n",
    "3. The algorithm works for any irreducible proposal chain.\n",
    "\n",
    "4. If the proposal chain is ergodic, then the resulting Metropolis-Hastings chain is also ergodic with limiting distribution $\\pi$.\n",
    "\n",
    "5. The generated sequence $X_0, X_1, \\dots$, gives an approximate sample from $\\pi$. However, if the chain requires many steps to get close to stationary, there may be initial bias. Burn-in refers to the practice of discarding the initial iterations and retaining $X_m, X_{m+1}, \\dots, X_n$ for some $m$. In that case, the strong LLN for Markov chains gives\n",
    "\n",
    "$$\\lim_{n\\to\\infty}\\frac{r(X_m) + \\dots + r(X_n)}{n-m+1} = \\sum_{x} r(x)\\pi_x.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis-Hastings for Continuous Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCMC can also be used in the continuous case, when $\\pi$ is a probability density function.\n",
    "\n",
    "For the continuous case, the transition matrix is replaced by a transition function, where $P_{ij}$ is the value of a conditional density function given $X_0 = i$.\n",
    "\n",
    "The algorithm is essentially the same as in the discrete case, with densities replacing discrete distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
